---
title: "Seminar 2: Large language models generate functional protein sequences across diverse families"
linkTitle: "Seminar 2"
date: '2023-08-26'
weight: 20
type: book
---

This week, please read the paper titled as "[Large language models generate functional protein sequences across diverse families](https://www.nature.com/articles/s41587-022-01618-2)" by Madani et. al.

## Abstract

Deep-learning language models have shown promise in various biotechnological applications, including protein design and engineering. Here we describe ProGen, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. The model was trained on 280 million protein sequences from >19,000 families and is augmented with control tags specifying protein properties. ProGen can be further fine-tuned to curated sequences and tags to improve controllable generation performance of proteins from families with sufficient homologous samples. Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4%. ProGen is readily adapted to diverse protein families, as we demonstrate with chorismate mutase and malate dehydrogenase.

## Assignment

 - Read the paper and using the [Question Sheet](https://ddls.aicell.io/uploads/JournalClub.pdf) which contains a set of questions designed to guide your reading and understanding.
 - Fill out your question sheet and [submit here](https://forms.gle/D9k6T2StgPttzbrr8) before the seminar.
 - Be ready to discuss in the seminar.
